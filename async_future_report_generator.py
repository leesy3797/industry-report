# __import__('pysqlite3')
# import sys
# sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

import datetime
import os
import re
import hashlib
import time
import asyncio 
from typing import List, Optional
import sqlite3
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

from langchain_chroma.vectorstores import Chroma
from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain.schema import Document
from langchain_community.document_loaders import UnstructuredURLLoader
import asyncio 
from langchain_text_splitters import RecursiveCharacterTextSplitter
import requests

# ì™¸ë¶€ ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ ì„í¬íŠ¸ (ì›ë³¸ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´)
# ì´ íŒŒì¼ ì™¸ë¶€ì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
from prompts import FUTURE_STRATEGY_ROADMAP_PROMPT
from async_report_generator import get_llm_model, initialize_reports_db, save_report_to_db, load_reports_from_db, _postprocess_report_output

load_dotenv() 

# --- í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---

def _mock_progress_callback(message, progress, status):
    """
    progress_callbackì´ ì œê³µë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì‚¬ìš©ë˜ëŠ” mock í•¨ìˆ˜
    """
    print(f"ğŸ“Š [Progress: {int(progress*100)}%] {status.upper()}: {message}")

@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=30))
async def _get_serper_results_with_retry(serper_search_tool, query_string: str, company_name: str, username: str, num_results=10) -> List[Document]:
    """
    Serper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê° ê²°ê³¼ì˜ ë§í¬ë¥¼ í†µí•´ ì›ë³¸ ì›¹ í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ê°€ì ¸ì™€
    LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"  [Serper ê²€ìƒ‰] '{query_string}' (ìƒìœ„ {num_results}ê°œ ê²°ê³¼)")
    
    results = serper_search_tool.results(query_string) 
    
    extracted_docs = []
    processed_links = set() 

    if 'organic' in results:
        for organic_result in results['organic']:
            link = organic_result.get('link')
            title = organic_result.get('title', 'N/A')
            snippet = organic_result.get('snippet', 'N/A')

            if link and link not in processed_links:
                try:
                    res = requests.get(link, timeout = 10)
                    loader = UnstructuredURLLoader(urls=[link])
                    documents = await loader.aload() 
                    
                    if documents and documents[0].page_content:
                        full_content = documents[0].page_content
                        full_content = re.sub(r'\s+', ' ', full_content).strip()
                        if len(full_content) < 100:
                            continue
                        
                        page_content = f"{full_content}"
                        metadata = {
                            "source": link,
                            "title": title,
                            "position": organic_result.get('position', -1),
                            "query_origin": query_string,
                            "fetched_type": "full_content",
                            "company_name": company_name,
                            "username": username  # âœ… ë©”íƒ€ë°ì´í„°ì— username ì¶”ê°€
                        }
                        extracted_docs.append(Document(page_content=page_content, metadata=metadata))
                        processed_links.add(link)
                    else:
                        if len(snippet) > 50:
                            page_content = f"ì œëª©: {title}\në‚´ìš©: {snippet} (ì›ë³¸ ë¡œë“œ ì‹¤íŒ¨)"
                            print(f"  (ì›ë³¸ ë¡œë“œ ì‹¤íŒ¨) ê²€ìƒ‰ì–´: {query_string} URL: {link}")
                            metadata = {
                                "source": link, "title": title, "position": organic_result.get('position', -1),
                                "query_origin": query_string, "fetched_type": "snippet_fallback",
                                "company_name": company_name,
                                "username": username  # âœ… ë©”íƒ€ë°ì´í„°ì— username ì¶”ê°€
                            }
                            extracted_docs.append(Document(page_content=page_content, metadata=metadata))
                except Exception as e:
                    if len(snippet) > 50:
                        page_content = f"ì œëª©: {title}\në‚´ìš©: {snippet} (ì›ë³¸ ë¡œë“œ ì‹¤íŒ¨)"
                        print(f"  (ì›ë³¸ ë¡œë“œ ì‹¤íŒ¨) ê²€ìƒ‰ì–´: {query_string} URL: {link}")
                        metadata = {
                            "source": link, "title": title, "position": organic_result.get('position', -1),
                            "query_origin": query_string, "fetched_type": "snippet_fallback",
                            "company_name": company_name,
                            "username": username  # âœ… ë©”íƒ€ë°ì´í„°ì— username ì¶”ê°€
                        }
                        extracted_docs.append(Document(page_content=page_content, metadata=metadata))
            else:
                if len(snippet) > 50:
                    page_content = f"ì œëª©: {title}\në‚´ìš©: {snippet} (ë§í¬ ì—†ê±°ë‚˜ ì¤‘ë³µ)"
                    metadata = {
                        "source": link if link else "N/A", "title": title, "position": organic_result.get('position', -1),
                        "query_origin": query_string, "fetched_type": "snippet_only",
                        "company_name": company_name,
                        "username": username  # âœ… ë©”íƒ€ë°ì´í„°ì— username ì¶”ê°€
                    }
                    extracted_docs.append(Document(page_content=page_content, metadata=metadata))

    # AnswerBox, KnowledgeGraph ë“± ìŠ¤ë‹ˆí« ì •ë³´ ì¶”ê°€
    if 'answerBox' in results and 'snippet' in results['answerBox']:
        ab_content = re.sub(r'\s+', ' ', results['answerBox']['snippet']).strip()
        page_content = f"AnswerBox ì œëª©: {results['answerBox'].get('title', 'N/A')}\në‚´ìš©: {ab_content}"
        metadata = {"source": results['answerBox'].get('link', 'N/A'), "title": results['answerBox'].get('title', 'N/A') or "Answer Box", "query_origin": query_string, "fetched_type": "answer_box_snippet", "company_name": company_name, "username": username} # âœ… ë©”íƒ€ë°ì´í„°ì— username ì¶”ê°€
        extracted_docs.append(Document(page_content=page_content, metadata=metadata))
    
    if 'knowledgeGraph' in results and 'snippet' in results['knowledgeGraph']:
        kg_content = re.sub(r'\s+', ' ', results['knowledgeGraph']['snippet']).strip()
        page_content = f"KnowledgeGraph ì œëª©: {results['knowledgeGraph'].get('title', 'N/A')}\në‚´ìš©: {kg_content}"
        metadata = {"source": results['knowledgeGraph'].get('link', 'N/A'), "title": results['knowledgeGraph'].get('title', 'N/A') or "Knowledge Graph", "query_origin": query_string, "fetched_type": "knowledge_graph_snippet", "company_name": company_name, "username": username} # âœ… ë©”íƒ€ë°ì´í„°ì— username ì¶”ê°€
        extracted_docs.append(Document(page_content=page_content, metadata=metadata))

    return extracted_docs


async def _generate_page_4_future_report(query: str, username: str, progress_callback=None, perform_serper_search: bool = True) -> str:
    """
    ì£¼ì–´ì§„ íšŒì‚¬(query)ì— ëŒ€í•œ ë¯¸ë˜ ì „ëµ ë¡œë“œë§µ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    - ì›¹ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (perform_serper_searchê°€ Trueì¼ ê²½ìš°)
    - ìˆ˜ì§‘ëœ ë¬¸ì„œë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ì„ ë³„í•©ë‹ˆë‹¤.
    - ì„ ë³„ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì„ í˜¸ì¶œí•˜ì—¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
    - ìµœì¢… ë³´ê³ ì„œë¥¼ DBì— ì €ì¥í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        query (str): ë¶„ì„í•  íšŒì‚¬ ì´ë¦„.
        username (str): ë³´ê³ ì„œë¥¼ ìš”ì²­í•œ ì‚¬ìš©ì ì´ë¦„.
        progress_callback (callable, optional): ì§„í–‰ ìƒí™©ì„ ì•Œë¦¬ëŠ” ì½œë°± í•¨ìˆ˜.
                                                (message, progress, status) í˜•íƒœ.
        perform_serper_search (bool): Serper ê²€ìƒ‰ì„ ìˆ˜í–‰í• ì§€ ì—¬ë¶€. Falseì´ë©´ ê¸°ì¡´ DBì—ì„œë§Œ ê²€ìƒ‰.
    Returns:
        str: ìƒì„±ëœ ë³´ê³ ì„œ ë‚´ìš©.
    """
    if progress_callback is None:
        progress_callback = _mock_progress_callback

    report_type = "future"
    current_year = datetime.datetime.now().year

    progress_callback(f"'{query}'ì— ëŒ€í•œ ë¯¸ë˜ ì „ëµ ë¡œë“œë§µ ë³´ê³ ì„œ ìƒì„± ì¤€ë¹„ ì¤‘...", 0.1, 'progress')
    
    try:
        # 1. LLM ë° DB ì´ˆê¸°í™”
        llm = get_llm_model() 
        await initialize_reports_db()

        # 2. DBì—ì„œ ê¸°ì¡´ ë³´ê³ ì„œ í™•ì¸
        existing_report_content = await load_reports_from_db(
            username=username, report_type=report_type, query=query, year=current_year
        )
        if existing_report_content:
            progress_callback(f"âœ… '{query}'ì— ëŒ€í•œ ê¸°ì¡´ ë³´ê³ ì„œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë³´ê³ ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.", 1.0, 'info')
            return existing_report_content

        progress_callback(f"ğŸ‘ ê¸°ì¡´ ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.", 0.15, 'info')

        # 3. Serper API ë° ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        serper_search = GoogleSerperAPIWrapper(
            gl='kr', hl='ko', serper_api_key=os.environ['SERPER_API_KEY']
        )
        vectorstore = Chroma(
            collection_name='future_report_search',
            # embedding_function=GoogleGenerativeAIEmbeddings(
            #     model='models/text-embedding-004',
            #     api_key=os.environ['GOOGLE_API_KEY']
            # ),
            embedding_function = OpenAIEmbeddings(
                model="text-embedding-3-small",
            ),
            persist_directory="./chroma_db"
        )
        progress_callback("âœ… ê²€ìƒ‰ ë„êµ¬ ë° ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ.", 0.2, 'progress')

        raw_serper_docs_for_vectorstore: List[Document] = []

        # ğŸš€ Serper ê²€ìƒ‰ ì¡°ê±´ë¶€ ì‹¤í–‰
        if perform_serper_search:
            progress_callback(f"ğŸ” Serper ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.", 0.25, 'progress')
            search_queries = [
                f"{query}ì˜ ë¯¸ë˜ ì „ëµ", 
                f"{query}ì˜ ì‹ ì‚¬ì—… ë™í–¥",
                f"{query}ì˜ ë¯¸ë˜ ë¨¹ê±°ë¦¬",
                f"{query}ì˜ ë¯¸ë˜ ì‹ ì‚¬ì—…"
            ]
            progress_callback(f"ğŸ” ì‚¬ì „ ì •ì˜ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ({len(search_queries)}ê°œ)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.", 0.25, 'progress')
            
            seen_serper_doc_hashes = set() # ì¤‘ë³µ ë¬¸ì„œ ë°©ì§€ë¥¼ ìœ„í•œ ì§‘í•©

            for i, search_q in enumerate(search_queries):
                current_progress = 0.25 + (0.35 * ((i + 1) / len(search_queries)))
                progress_callback(f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ({i+1}/{len(search_queries)}): '{search_q}'", current_progress, 'progress')
                try:
                    docs_for_query = await _get_serper_results_with_retry(serper_search, search_q, query, username) # âœ… username ì „ë‹¬
                    for doc in docs_for_query:
                        doc_hash = hashlib.sha256(doc.page_content.encode('utf-8')).hexdigest() 
                        if doc_hash not in seen_serper_doc_hashes:
                            raw_serper_docs_for_vectorstore.append(doc)
                            seen_serper_doc_hashes.add(doc_hash)
                except (RetryError, Exception) as e:
                    progress_callback(f"ì˜¤ë¥˜: '{search_q}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}", current_progress, 'warning')
                    continue

            if not raw_serper_docs_for_vectorstore:
                progress_callback(f"âš ï¸ '{query}'ì— ëŒ€í•œ ìƒˆë¡œìš´ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.", 0.6, 'warning')
            else:
                progress_callback(f"âœ… ì´ {len(raw_serper_docs_for_vectorstore)}ê°œì˜ ê³ ìœ í•œ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ.", 0.6, 'progress')

                # 5. ë¬¸ì„œ ë¶„í•  ë° ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ (ìƒˆë¡œìš´ ë¬¸ì„œë§Œ ì¶”ê°€)
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
                unique_docs_splits = text_splitter.split_documents(raw_serper_docs_for_vectorstore)
                
                # ChromaDBì— ì¶”ê°€í•˜ê¸° ì „ì— í˜„ì¬ DBì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” IDë¥¼ í•„í„°ë§
                current_db_ids = set(vectorstore.get(
                    where={"$and": [{"company_name": query}, {"username": username}]}, 
                    include=['metadatas']
                )['ids']) 
                documents_to_add = []
                ids_to_add = []
                
                for doc in unique_docs_splits:
                    # ë¬¸ì„œ ë‚´ìš©ê³¼ ì†ŒìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ  ID ìƒì„±
                    doc_id = hashlib.sha256(f"{doc.metadata.get('source', '')}_{doc.page_content}".encode('utf-8')).hexdigest()
                    if doc_id not in current_db_ids: # ğŸš€ ì´ë¯¸ DBì— ì—†ëŠ” IDë§Œ ì¶”ê°€
                        # metadataì— usernameì´ ìˆëŠ”ì§€ í™•ì¸ í›„ ì¶”ê°€
                        if "username" not in doc.metadata:
                            doc.metadata["username"] = username
                        documents_to_add.append(doc)
                        ids_to_add.append(doc_id)
                
                if documents_to_add:
                    progress_callback(f"ğŸ“¦ ì¤‘ë³µì„ ì œì™¸í•˜ê³  {len(documents_to_add)}ê°œì˜ ìƒˆë¡œìš´ ì²­í¬ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ ì¤‘...", 0.65, 'progress')
                    batch_size = 80
                    for i in range(0, len(documents_to_add), batch_size):
                        vectorstore.add_documents(
                            documents=documents_to_add[i : i + batch_size],
                            ids=ids_to_add[i : i + batch_size]
                        )
                        await asyncio.sleep(10) # ëŒ€ëŸ‰ ì¶”ê°€ í›„ ChromaDBì˜ ì•ˆì •ì„±ì„ ìœ„í•´ ì ì‹œ ëŒ€ê¸°
                    progress_callback("âœ… ìƒˆë¡œìš´ ê²€ìƒ‰ ê²°ê³¼ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ.", 0.7, 'progress')
                else:
                    progress_callback("â„¹ï¸ ìƒˆë¡œìš´ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (ëª¨ë‘ ì¤‘ë³µ).", 0.7, 'info')
        else:
            progress_callback("â­ï¸ Serper ê²€ìƒ‰ ë‹¨ê³„ê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.", 0.6, 'info')

        # 6. ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ë„ ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰
        # company_name ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•„í„°ë§
        retriever = vectorstore.as_retriever(
            search_type="mmr", 
            search_kwargs={
                "k": 20,
                "filter": {"$and": [{"company_name": query}, {"username": username}]} # âœ… í•„í„°ì— $and ì—°ì‚°ì ì¶”ê°€
            }
        )
        query_statement = f"{query}ì˜ í˜„ì¬ ì—­ëŸ‰, ë¯¸ë˜ ì„±ì¥ ë™ë ¥, ê¸°ìˆ  ë¡œë“œë§µ, ì¥ê¸°ì ì¸ ì‹œì¥ í¬ì§€ì…”ë‹ ë° ì‚¬ì—… í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™” ì „ëµì— ëŒ€í•œ í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ ìë£Œ"
        retrieved_docs = retriever.invoke(query_statement) 
        
        if not retrieved_docs:
            raise ValueError(f"âš ï¸ '{query}'ì— ëŒ€í•œ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œë¥¼ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Serper ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì—ˆê±°ë‚˜ ê¸°ì¡´ ë°ì´í„°ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        context_data = "\n\n".join([doc.page_content for doc in retrieved_docs])
        progress_callback(f"âœ… ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ {len(retrieved_docs)}ê°œ ì„ ë³„ ì™„ë£Œ.", 0.8, 'progress')
        
        # 7. LLMì„ í†µí•´ ë³´ê³ ì„œ ìƒì„±
        future_roadmap_prompt = PromptTemplate.from_template(FUTURE_STRATEGY_ROADMAP_PROMPT)
        future_roadmap_chain = future_roadmap_prompt | llm | StrOutputParser()
        roadmap_raw = future_roadmap_chain.invoke(
            {'company': query, 'context': context_data}
        )
        roadmap_content = _postprocess_report_output(roadmap_raw)

        # 8. ìƒì„±ëœ ë³´ê³ ì„œë¥¼ DBì— ì €ì¥
        await save_report_to_db( 
            username=username, report_type=report_type, query=query, year=current_year, month=None, content=roadmap_content # âœ… username ì „ë‹¬
        )
        
        progress_callback(f"ğŸ‰ '{query}'ì— ëŒ€í•œ ë¯¸ë˜ ì „ëµ ë¡œë“œë§µ ë³´ê³ ì„œ ìƒì„± ë° DB ì €ì¥ ì™„ë£Œ.", 1.0, 'info')
        return roadmap_content

    except Exception as e:
        message = f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}"
        progress_callback(message, 1.0, 'error')
        print(message)
        return message

if __name__ == '__main__':
    test_query = "cjëŒ€í•œí†µìš´"
    test_user_a = "ì´ìŠ¹ìš©" # âœ… ì‚¬ìš©ì A
    # test_user_b = "user_b" # âœ… ì‚¬ìš©ì B
    
    # ğŸš€ ì²« ë²ˆì§¸ ì‹¤í–‰: Serper ê²€ìƒ‰ ìˆ˜í–‰ (user_aë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘)
    print(f"\n--- ì²« ë²ˆì§¸ ì‹¤í–‰: '{test_query}' for '{test_user_a}' (Serper ê²€ìƒ‰ í¬í•¨) ---")
    final_report_first_run = asyncio.run(_generate_page_4_future_report(test_query, username=test_user_a, perform_serper_search=True))
    print("\n--- ì²« ë²ˆì§¸ ì‹¤í–‰ ìµœì¢… ë³´ê³ ì„œ ---")
    print(final_report_first_run)

    # ğŸš€ ë‘ ë²ˆì§¸ ì‹¤í–‰: Serper ê²€ìƒ‰ ê±´ë„ˆë›°ê¸° (user_aì˜ ê¸°ì¡´ DBì—ì„œ ê²€ìƒ‰)
    print(f"\n--- ë‘ ë²ˆì§¸ ì‹¤í–‰: '{test_query}' for '{test_user_a}' (Serper ê²€ìƒ‰ ê±´ë„ˆë›°ê³  ê¸°ì¡´ DB ì‚¬ìš©) ---")
    final_report_second_run = asyncio.run(_generate_page_4_future_report(test_query, username=test_user_a, perform_serper_search=False))
    print("\n--- ë‘ ë²ˆì§¸ ì‹¤í–‰ ìµœì¢… ë³´ê³ ì„œ ---")
    print(final_report_second_run)
    
    # # ğŸš€ ì„¸ ë²ˆì§¸ ì‹¤í–‰: Serper ê²€ìƒ‰ ìˆ˜í–‰ (user_bë¥¼ ìœ„í•œ ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘)
    # print(f"\n--- ì„¸ ë²ˆì§¸ ì‹¤í–‰: '{test_query}' for '{test_user_b}' (Serper ê²€ìƒ‰ í¬í•¨) ---")
    # final_report_third_run = asyncio.run(_generate_page_4_future_report(test_query, username=test_user_b, perform_serper_search=True))
    # print("\n--- ì„¸ ë²ˆì§¸ ì‹¤í–‰ ìµœì¢… ë³´ê³ ì„œ ---")
    # print(final_report_third_run)